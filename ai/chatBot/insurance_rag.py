from pydantic import BaseModel
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_postgres import PGVector
from langchain.prompts import PromptTemplate
from langchain_core.documents import Document
import os
import logging
import psycopg2
import json
from typing import List, Dict, Any, Optional

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# í™˜ê²½ ë³€ìˆ˜
DB_USER = os.getenv("DB_USER", "jjj")
DB_PASSWORD = os.getenv("DB_PASSWORD", "1q2w3e4r!")
DB_HOST = os.getenv("DB_HOST", "db")
DB_PORT = os.getenv("DB_PORT", "5432")
DB_NAME = os.getenv("DB_NAME", "meong")
CONNECTION_STRING = f"postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}"
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
EMBEDDING_MODEL_NAME = os.getenv("EMBEDDING_MODEL_NAME", "all-MiniLM-L6-v2")
VECTORSTORE_COLLECTION_NAME = os.getenv("INSURANCE_VECTORSTORE_COLLECTION_NAME", "insurance_vectors")
VECTORSTORE_DISTANCE_STRATEGY = os.getenv("VECTORSTORE_DISTANCE_STRATEGY", "cosine")
VECTORSTORE_SEARCH_LIMIT = int(os.getenv("INSURANCE_VECTORSTORE_SEARCH_LIMIT", "5"))
PROMPT_TEMPLATE_PATH = os.getenv("INSURANCE_PROMPT_TEMPLATE_PATH", "/app/chatBot/insurance_prompt_template.txt")

# ì„ë² ë”© ëª¨ë¸
try:
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
    logger.info(f"HuggingFaceEmbeddings initialized successfully with model: {EMBEDDING_MODEL_NAME}")
except Exception as e:
    logger.error(f"Failed to initialize HuggingFaceEmbeddings: {e}")
    raise Exception("Embedding initialization failed")

# PGVector ì´ˆê¸°í™”
try:
    logger.debug(f"Attempting to connect to database with: {CONNECTION_STRING}")
    vectorstore = PGVector(
        connection=CONNECTION_STRING,
        embeddings=embeddings,
        collection_name=VECTORSTORE_COLLECTION_NAME,
        distance_strategy=VECTORSTORE_DISTANCE_STRATEGY,
        use_jsonb=True,
        pre_delete_collection=False  # ê¸°ì¡´ ì»¬ë ‰ì…˜ ìœ ì§€
    )
    logger.info("PGVector initialized successfully")
except Exception as e:
    logger.error(f"Failed to initialize PGVector: {e}", exc_info=True)
    raise Exception(f"Vectorstore initialization failed: {str(e)}")

# ë³´í—˜ ìƒí’ˆ ë°ì´í„°ë¥¼ ë²¡í„°ìŠ¤í† ì–´ì— ì‚½ì…
def initialize_insurance_vectorstore():
    try:
        logger.debug("Starting initialize_insurance_vectorstore")
        
        # ê¸°ì¡´ ë°ì´í„° í™•ì¸ ë° ì‚­ì œ
        with psycopg2.connect(
            host=DB_HOST,
            port=DB_PORT,
            dbname=DB_NAME,
            user=DB_USER,
            password=DB_PASSWORD
        ) as conn:
            with conn.cursor() as cur:
                cur.execute("""
                    SELECT COUNT(*)
                    FROM langchain_pg_embedding e
                    JOIN langchain_pg_collection c ON e.collection_id = c.uuid
                    WHERE c.name = %s
                """, (VECTORSTORE_COLLECTION_NAME,))
                count = cur.fetchone()[0]
                logger.debug(f"Existing records in {VECTORSTORE_COLLECTION_NAME}: {count}")

                if count > 0:
                    logger.info(f"Deleting existing data in {VECTORSTORE_COLLECTION_NAME}")
                    cur.execute("""
                        DELETE FROM langchain_pg_embedding
                        WHERE collection_id = (
                            SELECT uuid FROM langchain_pg_collection WHERE name = %s
                        )
                    """, (VECTORSTORE_COLLECTION_NAME,))
                    conn.commit()
                    logger.info("Existing data deleted")

        # ë³´í—˜ ìƒí’ˆ ë°ì´í„°ë¥¼ DBì—ì„œ ê°€ì ¸ì™€ì„œ ë²¡í„°ìŠ¤í† ì–´ì— ì‚½ì…
        # logger.info("Fetching insurance products from database")
        insurance_docs = fetch_insurance_products_from_db()
        
        if insurance_docs:
            try:
                # ê¸°ì¡´ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                existing_docs = vectorstore.similarity_search("ë³´í—˜", k=1)
                if existing_docs:
                    pass  # logger.info("Insurance data already exists in vectorstore, skipping insertion")
                else:
                    vectorstore.add_documents(insurance_docs)
                    # logger.info(f"Inserted {len(insurance_docs)} insurance products into vectorstore")
            except Exception as e:
                logger.warning(f"Failed to add insurance documents to vectorstore: {e}")
                logger.info("Continuing without insurance vectorstore initialization")
        else:
            logger.warning("No insurance products found in database")
            
    except Exception as e:
        logger.error(f"Failed to initialize insurance vectorstore: {str(e)}")
        raise Exception(f"Insurance vectorstore initialization failed: {str(e)}")

# DBì—ì„œ ë³´í—˜ ìƒí’ˆ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ Document í˜•íƒœë¡œ ë³€í™˜
def fetch_insurance_products_from_db() -> List[Document]:
    try:
        with psycopg2.connect(
            host=DB_HOST,
            port=DB_PORT,
            dbname=DB_NAME,
            user=DB_USER,
            password=DB_PASSWORD
        ) as conn:
            with conn.cursor() as cur:
                cur.execute("""
                    SELECT 
                        id, company, product_name, description, 
                        features, coverage_details, redirect_url, logo_url
                    FROM insurance_products 
                    ORDER BY id
                """)
                
                rows = cur.fetchall()
                documents = []
                
                for row in rows:
                    id, company, product_name, description, features, coverage_details, redirect_url, logo_url = row
                    
                    # ë³´ì¥ë‚´ì—­ê³¼ íŠ¹ì§•ì„ íŒŒì‹±
                    features_list = features.split(',') if features else []
                    coverage_list = coverage_details.split(',') if coverage_details else []
                    
                    # Document ë‚´ìš© êµ¬ì„±
                    content = f"""
ë³´í—˜ì‚¬: {company}
ìƒí’ˆëª…: {product_name}
ì„¤ëª…: {description}
ì£¼ìš” íŠ¹ì§•: {', '.join(features_list[:5])}  # ìƒìœ„ 5ê°œë§Œ
ë³´ì¥ë‚´ì—­: {', '.join(coverage_list[:10])}  # ìƒìœ„ 10ê°œë§Œ
ê°€ì…ë§í¬: {redirect_url}
                    """.strip()
                    
                    # ë©”íƒ€ë°ì´í„° êµ¬ì„±
                    metadata = {
                        "source": "insurance_database",
                        "id": id,
                        "company": company,
                        "product_name": product_name,
                        "features": features_list,
                        "coverage_details": coverage_list,
                        "redirect_url": redirect_url,
                        "logo_url": logo_url
                    }
                    
                    documents.append(Document(
                        page_content=content,
                        metadata=metadata
                    ))
                
                logger.info(f"Fetched {len(documents)} insurance products from database")
                return documents
                
    except Exception as e:
        logger.error(f"Failed to fetch insurance products from database: {str(e)}")
        return []

# OpenAI ëª¨ë¸
try:
    llm = ChatOpenAI(
        api_key=OPENAI_API_KEY,
        model=OPENAI_MODEL,
        max_tokens=300  # ë³´í—˜ ì •ë³´ëŠ” ë” ê¸´ ì‘ë‹µì´ í•„ìš”í•  ìˆ˜ ìˆìŒ
    )
    logger.info("ChatOpenAI initialized successfully")
except Exception as e:
    logger.error(f"Failed to initialize ChatOpenAI: {e}")
    raise Exception("LLM initialization failed")

# ë³´í—˜ ì „ìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
def get_insurance_prompt_template():
    try:
        with open(PROMPT_TEMPLATE_PATH, 'r', encoding='utf-8') as f:
            prompt_template_content = f.read()
        prompt_template = PromptTemplate(input_variables=["context", "query"], template=prompt_template_content)
        logger.info(f"Insurance prompt template loaded successfully from {PROMPT_TEMPLATE_PATH}")
        return prompt_template
    except Exception as e:
        logger.error(f"Failed to load insurance prompt template from {PROMPT_TEMPLATE_PATH}: {e}")
        # ê¸°ë³¸ í…œí”Œë¦¿ ì‚¬ìš©
        default_template = """
ë‹¹ì‹ ì€ í«ë³´í—˜ ì „ë¬¸ê°€ ì±—ë´‡ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ìœ„ Contextë¥¼ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.

[Context]
{context}

[Instruction]
- ë³´í—˜ì‚¬ëª…ê³¼ ìƒí’ˆëª…ì„ ëª…í™•íˆ í¬í•¨í•˜ì„¸ìš”
- ì£¼ìš” ë³´ì¥ ë‚´ì—­ì„ ìš”ì•½í•´ì„œ ì„¤ëª…í•˜ì„¸ìš”
- ê° ìƒí’ˆì˜ ê°€ì… ë§í¬ê°€ ìˆë‹¤ë©´ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”
- ë§í¬ëŠ” "ğŸ”— [ë³´í—˜ì‚¬ëª…] ë°”ë¡œê°€ê¸°: [ë§í¬]" í˜•ì‹ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”
- ì‚¬ìš©ì ì¹œí™”ì ì¸ í†¤ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
- 300ì ì´ë‚´ë¡œ ë‹µë³€í•˜ì„¸ìš”
- ë³´í—˜ ì „ë¬¸ ìš©ì–´ëŠ” ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…í•˜ì„¸ìš”

[ì£¼ì˜ì‚¬í•­]
- Contextì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”
- ë³´í—˜ ê°€ì…ì„ ê°•ìš”í•˜ì§€ ë§ˆì„¸ìš”
- ê°ê´€ì ì´ê³  ì •í™•í•œ ì •ë³´ë§Œ ì œê³µí•˜ì„¸ìš”
- ë§í¬ê°€ ìˆëŠ” ìƒí’ˆì€ ë°˜ë“œì‹œ ë§í¬ë¥¼ í¬í•¨í•˜ì„¸ìš”

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ë‹µë³€:
        """
        return PromptTemplate(input_variables=["context", "query"], template=default_template)

class InsuranceQueryRequest(BaseModel):
    query: str
    petId: Optional[int] = None

    class Config:
        # null ê°’ì„ í—ˆìš©í•˜ë„ë¡ ì„¤ì •
        json_encoders = {
            int: lambda v: v if v is not None else None
        }

async def process_insurance_rag_query(query: str, pet_id: Optional[int] = None):
    try:
        # í•œê¸€ ì¸ì½”ë”© í™•ì¸ ë° ë¡œê¹…
        logger.info(f"Processing insurance query (raw): {repr(query)}")
        logger.info(f"Processing insurance query (decoded): {query}")
        logger.info(f"Received petId: {pet_id}")
        
        # petIdê°€ ìˆìœ¼ë©´ MyPet ì •ë³´ë¥¼ í¬í•¨í•œ ì¿¼ë¦¬ë¡œ ì²˜ë¦¬
        if pet_id:
            # MyPet ì •ë³´ë¥¼ ê°€ì ¸ì™€ì„œ ì¿¼ë¦¬ì— í¬í•¨
            from main import get_mypet_info
            pet_info = await get_mypet_info(pet_id)
            if pet_info:
                # í« ì •ë³´ ì¶”ì¶œ
                pet_name = pet_info.get('name', '')
                breed = pet_info.get('breed', '')
                age = pet_info.get('age', '')
                gender = pet_info.get('gender', '')
                weight = pet_info.get('weight', '')
                
                # ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ @íƒœê·¸ë¥¼ í« ì •ë³´ë¡œ ì¹˜í™˜
                import re
                pet_tag_pattern = r'@' + re.escape(pet_name)
                enhanced_query = re.sub(pet_tag_pattern, f"{breed} {age}ì„¸ {gender}", query)
                
                # í« ì •ë³´ë¥¼ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
                final_query = f"""
í« ì •ë³´:
- ì´ë¦„: {pet_name}
- í’ˆì¢…: {breed}
- ë‚˜ì´: {age}ì„¸
- ì„±ë³„: {gender}
- ì²´ì¤‘: {weight}kg

ì‚¬ìš©ì ì§ˆë¬¸: {enhanced_query}
                """.strip()
                logger.info(f"Enhanced query with pet tag replacement: {final_query}")
            else:
                final_query = query
        else:
            final_query = query
        
        # ì§ì ‘ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë³´í—˜ ìƒí’ˆ ê²€ìƒ‰
        insurance_products = fetch_insurance_products_from_db()
        
        if not insurance_products:
            return {"answer": "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë“±ë¡ëœ ë³´í—˜ ìƒí’ˆ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        # ê³ ê¸‰ í•„í„°ë§ ì‹œìŠ¤í…œ
        filtered_products = filter_insurance_products(insurance_products, final_query)
        
        if not filtered_products:
            return {"answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ê²€ìƒ‰ ì¡°ê±´ì— ë§ëŠ” ë³´í—˜ ìƒí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¡œ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”."}
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_parts = []
        for i, product in enumerate(filtered_products, 1):
            metadata = product.metadata
            company = metadata.get('company', '')
            product_name = metadata.get('product_name', '')
            redirect_url = metadata.get('redirect_url', '')
            
            # ë§í¬ ì •ë³´ë¥¼ í¬í•¨í•œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            product_info = f"[ìƒí’ˆ {i}]\n{product.page_content}"
            if redirect_url:
                product_info += f"\nğŸ”— ê°€ì… ë§í¬: {redirect_url}"
            
            context_parts.append(product_info)
        
        context = "\n\n".join(context_parts)
        logger.info(f"Retrieved insurance context: {context[:200]}...")  # ì»¨í…ìŠ¤íŠ¸ ì•ë¶€ë¶„ë§Œ ë¡œê¹…
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„± ë° LLM í˜¸ì¶œ
        prompt_template = get_insurance_prompt_template()
        prompt = prompt_template.format(context=context, query=final_query)
        logger.info(f"Generated insurance prompt: {prompt[:200]}...")  # í”„ë¡¬í”„íŠ¸ ì•ë¶€ë¶„ë§Œ ë¡œê¹…
        
        response = llm.invoke(prompt)
        logger.info(f"Insurance LLM response: {response.content}")
        
        return {"answer": response.content}
        
    except Exception as e:
        logger.error(f"Error processing insurance query: {e}", exc_info=True)
        raise Exception(f"Insurance query processing failed: {str(e)}")

def filter_insurance_products(products, query):
    """
    ê²€ìƒ‰ì–´ì— ë”°ë¼ ë³´í—˜ ìƒí’ˆì„ í•„í„°ë§í•˜ëŠ” ê³ ê¸‰ ì‹œìŠ¤í…œ
    """
    query_lower = query.lower()
    filtered_products = []
    
    # ê²€ìƒ‰ ì¡°ê±´ ì •ì˜
    search_conditions = {
        'ë³´í—˜ì‚¬': {
            'ì‚¼ì„±í™”ì¬': ['ì‚¼ì„±', 'ì‚¼ì„±í™”ì¬', 'samsung'],
            'NHë†í˜‘ì†í•´ë³´í—˜': ['nh', 'ë†í˜‘', 'ë†í˜‘ì†í•´ë³´í—˜', 'nhë†í˜‘'],
            'KBì†í•´ë³´í—˜': ['kb', 'êµ­ë¯¼', 'kbì†í•´ë³´í—˜'],
            'í˜„ëŒ€í•´ìƒ': ['í˜„ëŒ€', 'í˜„ëŒ€í•´ìƒ', 'hi'],
            'ë©”ë¦¬ì¸ í™”ì¬': ['ë©”ë¦¬ì¸ ', 'meritz'],
            'DBì†í•´ë³´í—˜': ['db', 'dbì†í•´ë³´í—˜']
        },
        'ê°€ì…ì¡°ê±´': {
            'ë‚˜ì´': ['ë‚˜ì´', 'ì—°ë ¹', 'ë§Œë‚˜ì´', 'ìƒí›„', 'ê°œì›”', 'ì„¸'],
            'ì¢…': ['ê°•ì•„ì§€', 'ê³ ì–‘ì´', 'ë°˜ë ¤ê²¬', 'ë°˜ë ¤ë¬˜', 'ê°œ', 'ê³ ì–‘ì´'],
            'í’ˆì¢…': ['í’ˆì¢…', 'ê²¬ì¢…', 'ë¬˜ì¢…']
        },
        'ë³´ì¥ë‚´ì—­': {
            'ì˜ë£Œë¹„': ['ì˜ë£Œë¹„', 'ì¹˜ë£Œë¹„', 'ë³‘ì›ë¹„', 'ì§„ë£Œë¹„'],
            'ìˆ˜ìˆ ë¹„': ['ìˆ˜ìˆ ë¹„', 'ìˆ˜ìˆ ', 'ì™¸ê³¼'],
            'ì…ì›': ['ì…ì›', 'ì…ì›ë¹„', 'ì…ì›ì¹˜ë£Œ'],
            'í†µì›': ['í†µì›', 'í†µì›ì¹˜ë£Œ', 'ì™¸ë˜'],
            'ê²€ì‚¬ë¹„': ['ê²€ì‚¬ë¹„', 'ê²€ì‚¬', 'ì§„ë‹¨'],
            'ì•½í’ˆë¹„': ['ì•½í’ˆë¹„', 'ì•½', 'ì²˜ë°©']
        },
        'íŠ¹ë³„ì¡°ê±´': {
            'íŠ¹ì•½': ['íŠ¹ì•½', 'ì¶”ê°€ë³´ì¥', 'ì„ íƒë³´ì¥'],
            'í• ì¸': ['í• ì¸', 'í˜œíƒ', 'ì´ë²¤íŠ¸'],
            'ìë™ê°±ì‹ ': ['ê°±ì‹ ', 'ìë™ê°±ì‹ ', 'ì—°ì¥']
        }
    }
    
    for product in products:
        score = 0
        product_text = product.page_content.lower()
        metadata = product.metadata
        
        # 1. ë³´í—˜ì‚¬ í•„í„°ë§
        company = metadata.get('company', '').lower()
        for company_name, keywords in search_conditions['ë³´í—˜ì‚¬'].items():
            if any(keyword in query_lower for keyword in keywords):
                if company_name.lower() in company:
                    score += 10  # ë†’ì€ ì ìˆ˜
                    break
        
        # 2. ê°€ì…ì¡°ê±´ í•„í„°ë§
        for condition_type, keywords in search_conditions['ê°€ì…ì¡°ê±´'].items():
            if any(keyword in query_lower for keyword in keywords):
                if any(keyword in product_text for keyword in keywords):
                    score += 8
        
        # 3. ë³´ì¥ë‚´ì—­ í•„í„°ë§
        for coverage_type, keywords in search_conditions['ë³´ì¥ë‚´ì—­'].items():
            if any(keyword in query_lower for keyword in keywords):
                if any(keyword in product_text for keyword in keywords):
                    score += 6
        
        # 4. íŠ¹ë³„ì¡°ê±´ í•„í„°ë§
        for special_type, keywords in search_conditions['íŠ¹ë³„ì¡°ê±´'].items():
            if any(keyword in query_lower for keyword in keywords):
                if any(keyword in product_text for keyword in keywords):
                    score += 4
        
        # 5. ì¼ë°˜ í‚¤ì›Œë“œ ë§¤ì¹­
        general_keywords = ['ë³´í—˜', 'í«ë³´í—˜', 'ë™ë¬¼ë³´í—˜', 'ê°€ì…', 'ë³´ì¥', 'ë³´ìƒ', 'ë³´í—˜ë£Œ', 'ìƒí’ˆ']
        for keyword in general_keywords:
            if keyword in query_lower and keyword in product_text:
                score += 2
        
        # 6. ì •í™•í•œ ë¬¸êµ¬ ë§¤ì¹­ (ë†’ì€ ì ìˆ˜)
        if query_lower in product_text:
            score += 15
        
        # 7. ì œí’ˆëª… ë§¤ì¹­
        product_name = metadata.get('product_name', '').lower()
        if query_lower in product_name:
            score += 12
        
        # ì ìˆ˜ê°€ ìˆëŠ” ìƒí’ˆë§Œ í•„í„°ë§
        if score > 0:
            filtered_products.append((score, product))
    
    # ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬
    filtered_products.sort(key=lambda x: x[0], reverse=True)
    
    # ìƒìœ„ 3ê°œ ìƒí’ˆ ë°˜í™˜
    return [product for score, product in filtered_products[:3]]

# ì„œë²„ ì‹œì‘ ì‹œ ë³´í—˜ ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™”
initialize_insurance_vectorstore() 