# -*- coding: utf-8 -*-
"""
ê°•ì•„ì§€ ê°ì • ë¶„ë¥˜ ëª¨ë¸ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸
PyTorchë¥¼ í™œìš©í•œ ì™„ì „í•œ í•™ìŠµ íŒŒì´í”„ë¼ì¸
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau
import time
import os
import json
from pathlib import Path
import numpy as np
from tqdm import tqdm
import sys
import io

# Windows ì½˜ì†”ì—ì„œ í•œê¸€ ì¶œë ¥ (ê¸°ë³¸ ì¸ì½”ë”© ì‚¬ìš©)

# ì‹œì‘ ì•Œë¦¼
print("ğŸš€ ê°•ì•„ì§€ ê°ì • ë¶„ë¥˜ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘!")

# ë¡œì»¬ ëª¨ë“ˆ import
import sys
from pathlib import Path

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€
try:
    current_dir = Path(__file__).parent
except NameError:
    # __file__ì´ ì •ì˜ë˜ì§€ ì•Šì€ ê²½ìš° í˜„ì¬ ë””ë ‰í† ë¦¬ ì‚¬ìš©
    current_dir = Path.cwd() / "emotion"

sys.path.insert(0, str(current_dir))

print(f"ğŸ“ í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {current_dir}")
print(f"ğŸ“ ì‹¤ì œ ê²½ë¡œ ì¡´ì¬ í™•ì¸: {current_dir.exists()}")

try:
    print("ğŸ“¦ ëª¨ë“ˆ import ì¤‘...")
    from training_model import create_model
    from dataset import create_data_loaders
    print("âœ… ëª¨ë“ˆ import ì„±ê³µ!")
except ImportError as e:
    print(f"âŒ Import ì˜¤ë¥˜: {e}")
    print("ğŸ“ íŒŒì¼ ì¡´ì¬ í™•ì¸:")
    print(f"   training_model.py: {(current_dir / 'training_model.py').exists()}")
    print(f"   dataset.py: {(current_dir / 'dataset.py').exists()}")
    raise

class DogEmotionTrainer:
    """
    ê°•ì•„ì§€ ê°ì • ë¶„ë¥˜ ëª¨ë¸ í›ˆë ¨ í´ë˜ìŠ¤
    """
    
    def __init__(self, config):
        """
        í›ˆë ¨ê¸° ì´ˆê¸°í™”
        
        Args:
            config (dict): í›ˆë ¨ ì„¤ì •
        """
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ–¥ï¸  ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.save_dir = Path(config['save_dir'])
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        # í›ˆë ¨ ê¸°ë¡ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        self.train_losses = []
        self.val_losses = []
        self.train_accuracies = []
        self.val_accuracies = []
        
        # ëª¨ë¸, ì˜µí‹°ë§ˆì´ì €, ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™”
        self._setup_model()
        self._setup_optimizer()
        self._setup_scheduler()
        
        # Early Stopping ì„¤ì •
        self.best_val_loss = float('inf')
        self.best_val_acc = 0.0
        self.patience_counter = 0
        
    def _setup_model(self):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        print("ğŸ¤– ëª¨ë¸ ì„¤ì • ì¤‘...")
        
        self.model = create_model(
            num_classes=self.config['num_classes'],
            pretrained=self.config['pretrained'],
            dropout_rate=self.config['dropout_rate'],
            freeze_backbone=self.config['freeze_backbone']
        )
        
        self.model.to(self.device)
        
        # ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •
        if self.config.get('use_class_weights', False):
            # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ëŠ” DataLoaderì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨
            self.criterion = nn.CrossEntropyLoss()
        else:
            self.criterion = nn.CrossEntropyLoss()
            
        print(f"âœ… ëª¨ë¸ì´ {self.device}ì— ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def _setup_optimizer(self):
        """ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”"""
        optimizer_name = self.config['optimizer'].lower()
        lr = self.config['learning_rate']
        weight_decay = self.config.get('weight_decay', 1e-4)
        
        if optimizer_name == 'adam':
            self.optimizer = optim.Adam(
                self.model.parameters(), 
                lr=lr, 
                weight_decay=weight_decay
            )
        elif optimizer_name == 'sgd':
            self.optimizer = optim.SGD(
                self.model.parameters(), 
                lr=lr, 
                momentum=0.9, 
                weight_decay=weight_decay
            )
        elif optimizer_name == 'adamw':
            self.optimizer = optim.AdamW(
                self.model.parameters(), 
                lr=lr, 
                weight_decay=weight_decay
            )
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì˜µí‹°ë§ˆì´ì €: {optimizer_name}")
        
        print(f"ğŸ“Š ì˜µí‹°ë§ˆì´ì €: {optimizer_name.upper()}, í•™ìŠµë¥ : {lr}")
    
    def _setup_scheduler(self):
        """í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™”"""
        scheduler_type = self.config.get('scheduler_type', 'step')
        
        if scheduler_type == 'step':
            self.scheduler = StepLR(
                self.optimizer, 
                step_size=self.config.get('step_size', 10),
                gamma=self.config.get('gamma', 0.1)
            )
        elif scheduler_type == 'reduce_on_plateau':
            self.scheduler = ReduceLROnPlateau(
                self.optimizer, 
                mode='min',
                patience=self.config.get('scheduler_patience', 5),
                factor=0.5,
                verbose=True
            )
        else:
            self.scheduler = None
        
        if self.scheduler:
            print(f"ğŸ“ˆ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬: {scheduler_type}")
    
    def train_epoch(self, train_loader):
        """í•œ ì—í¬í¬ í›ˆë ¨"""
        self.model.train()
        
        running_loss = 0.0
        correct = 0
        total = 0
        
        # ì§„í–‰ë°” ì„¤ì •
        pbar = tqdm(train_loader, desc="í›ˆë ¨ ì¤‘", leave=False)
        
        for batch_idx, (images, labels, _) in enumerate(pbar):
            images, labels = images.to(self.device), labels.to(self.device)
            
            # ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”
            self.optimizer.zero_grad()
            
            # ìˆœì „íŒŒ
            outputs = self.model(images)
            loss = self.criterion(outputs, labels)
            
            # ì—­ì „íŒŒ ë° ì˜µí‹°ë§ˆì´ì € ìŠ¤í…
            loss.backward()
            self.optimizer.step()
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            
            # ì§„í–‰ë°” ì—…ë°ì´íŠ¸
            current_loss = running_loss / (batch_idx + 1)
            current_acc = 100. * correct / total
            pbar.set_postfix({
                'Loss': f'{current_loss:.4f}',
                'Acc': f'{current_acc:.2f}%'
            })
        
        epoch_loss = running_loss / len(train_loader)
        epoch_acc = 100. * correct / total
        
        return epoch_loss, epoch_acc
    
    def validate_epoch(self, val_loader):
        """í•œ ì—í¬í¬ ê²€ì¦"""
        self.model.eval()
        
        running_loss = 0.0
        correct = 0
        total = 0
        
        # í´ë˜ìŠ¤ë³„ ì •í™•ë„ ê³„ì‚°ì„ ìœ„í•œ ë³€ìˆ˜
        class_correct = list(0. for i in range(self.config['num_classes']))
        class_total = list(0. for i in range(self.config['num_classes']))
        
        with torch.no_grad():
            pbar = tqdm(val_loader, desc="ê²€ì¦ ì¤‘", leave=False)
            
            for images, labels, _ in pbar:
                images, labels = images.to(self.device), labels.to(self.device)
                
                outputs = self.model(images)
                loss = self.criterion(outputs, labels)
                
                running_loss += loss.item()
                _, predicted = torch.max(outputs, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
                
                # í´ë˜ìŠ¤ë³„ ì •í™•ë„ ê³„ì‚°
                c = (predicted == labels).squeeze()
                for i in range(labels.size(0)):
                    label = labels[i]
                    class_correct[label] += c[i].item()
                    class_total[label] += 1
                
                # ì§„í–‰ë°” ì—…ë°ì´íŠ¸
                current_loss = running_loss / (len(pbar.container) if hasattr(pbar, 'container') else 1)
                current_acc = 100. * correct / total
                pbar.set_postfix({
                    'Loss': f'{current_loss:.4f}',
                    'Acc': f'{current_acc:.2f}%'
                })
        
        epoch_loss = running_loss / len(val_loader)
        epoch_acc = 100. * correct / total
        
        # í´ë˜ìŠ¤ë³„ ì •í™•ë„ ì¶œë ¥
        emotion_labels = ['angry', 'happy', 'relaxed', 'sad']
        class_accuracies = {}
        
        print("\nğŸ“Š í´ë˜ìŠ¤ë³„ ê²€ì¦ ì •í™•ë„:")
        for i in range(self.config['num_classes']):
            if class_total[i] > 0:
                acc = 100 * class_correct[i] / class_total[i]
                class_accuracies[emotion_labels[i]] = acc
                print(f"   {emotion_labels[i]:>8}: {acc:.2f}% ({int(class_correct[i])}/{int(class_total[i])})")
        
        return epoch_loss, epoch_acc, class_accuracies
    
    def train(self, train_loader, val_loader):
        """ì „ì²´ í›ˆë ¨ í”„ë¡œì„¸ìŠ¤"""
        print(f"\nğŸš€ ëª¨ë¸ í›ˆë ¨ ì‹œì‘!")
        print(f"ğŸ“Š ì—í¬í¬ ìˆ˜: {self.config['num_epochs']}")
        print(f"ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {self.config['batch_size']}")
        print(f"ğŸ¯ ì¡°ê¸° ì¢…ë£Œ patience: {self.config['early_stopping_patience']}")
        print("=" * 60)
        
        start_time = time.time()
        
        for epoch in range(self.config['num_epochs']):
            epoch_start_time = time.time()
            
            print(f"\nğŸ“… Epoch [{epoch+1}/{self.config['num_epochs']}]")
            
            # í›ˆë ¨
            train_loss, train_acc = self.train_epoch(train_loader)
            
            # ê²€ì¦
            val_loss, val_acc, class_accuracies = self.validate_epoch(val_loader)
            
            # ê¸°ë¡ ì €ì¥
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            self.train_accuracies.append(train_acc)
            self.val_accuracies.append(val_acc)
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            if self.scheduler:
                if isinstance(self.scheduler, ReduceLROnPlateau):
                    self.scheduler.step(val_loss)
                else:
                    self.scheduler.step()
            
            # ì—í¬í¬ ê²°ê³¼ ì¶œë ¥
            epoch_time = time.time() - epoch_start_time
            current_lr = self.optimizer.param_groups[0]['lr']
            
            print(f"\nâ±ï¸  ì—í¬í¬ ì‹œê°„: {epoch_time:.2f}ì´ˆ")
            print(f"ğŸ“š í›ˆë ¨   - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%")
            print(f"ğŸ“Š ê²€ì¦   - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%")
            print(f"ğŸ“ˆ í•™ìŠµë¥ : {current_lr:.6f}")
            
            # ìµœê³  ëª¨ë¸ ì €ì¥
            is_best = val_acc > self.best_val_acc
            if is_best:
                self.best_val_acc = val_acc
                self.best_val_loss = val_loss
                self.save_checkpoint(epoch, is_best=True)
                print(f"âœ… ìƒˆë¡œìš´ ìµœê³  ê²€ì¦ ì •í™•ë„: {val_acc:.2f}%")
                self.patience_counter = 0
            else:
                self.patience_counter += 1
            
            # ì •ê¸°ì ìœ¼ë¡œ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            if (epoch + 1) % self.config.get('save_frequency', 10) == 0:
                self.save_checkpoint(epoch, is_best=False)
            
            # Early Stopping ì²´í¬
            if self.patience_counter >= self.config['early_stopping_patience']:
                print(f"\nğŸ›‘ Early Stopping! {self.config['early_stopping_patience']} ì—í¬í¬ ë™ì•ˆ ê°œì„  ì—†ìŒ")
                break
        
        # í›ˆë ¨ ì™„ë£Œ
        total_time = time.time() - start_time
        print(f"\nğŸ‰ í›ˆë ¨ ì™„ë£Œ!")
        print(f"â±ï¸  ì´ í›ˆë ¨ ì‹œê°„: {total_time/60:.2f}ë¶„")
        print(f"ğŸ† ìµœê³  ê²€ì¦ ì •í™•ë„: {self.best_val_acc:.2f}%")
        
        # ìµœì¢… ê²°ê³¼ ì €ì¥
        self.save_training_history()
        
        return self.train_losses, self.val_losses, self.train_accuracies, self.val_accuracies
    
    def save_checkpoint(self, epoch, is_best=False):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint = {
            'epoch': epoch + 1,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'train_accuracies': self.train_accuracies,
            'val_accuracies': self.val_accuracies,
            'best_val_acc': self.best_val_acc,
            'config': self.config
        }
        
        # ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        checkpoint_path = self.save_dir / 'last_checkpoint.pth'
        torch.save(checkpoint, checkpoint_path)
        
        # ìµœê³  ëª¨ë¸ ì €ì¥
        if is_best:
            best_path = self.save_dir / 'best_model.pth'
            torch.save(checkpoint, best_path)
    
    def save_training_history(self):
        """í›ˆë ¨ ê¸°ë¡ ì €ì¥"""
        history = {
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'train_accuracies': self.train_accuracies,
            'val_accuracies': self.val_accuracies,
            'best_val_acc': self.best_val_acc,
            'config': self.config
        }
        
        history_path = self.save_dir / 'training_history.json'
        with open(history_path, 'w', encoding='utf-8') as f:
            json.dump(history, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ í›ˆë ¨ ê¸°ë¡ì´ {history_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def create_training_config():
    """ê¸°ë³¸ í›ˆë ¨ ì„¤ì • ìƒì„±"""
    config = {
        # ëª¨ë¸ ì„¤ì • - Fine-tuning
        'num_classes': 4,
        'pretrained': True,
        'dropout_rate': 0.3,          # ë“œë¡­ì•„ì›ƒ ê°ì†Œ (0.5 â†’ 0.3)
        'freeze_backbone': False,     # ë°±ë³¸ í•´ì œ (Fine-tuning)
        
        # í›ˆë ¨ ì„¤ì • - Fine-tuningìš©
        'num_epochs': 30,             # ì—í¬í¬ ê°ì†Œ (50 â†’ 30)
        'batch_size': 32,
        'learning_rate': 0.0001,      # í•™ìŠµë¥  ê°ì†Œ (0.001 â†’ 0.0001)
        'weight_decay': 1e-4,
        'optimizer': 'adam',
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        'scheduler_type': 'reduce_on_plateau',
        'step_size': 10,
        'gamma': 0.1,
        'scheduler_patience': 3,      # ë” ë¹ ë¥¸ í•™ìŠµë¥  ê°ì†Œ
        
        # ë°ì´í„° ì„¤ì •
        'image_size': 224,
        'num_workers': 2,             # Windows ì•ˆì •ì„±ì„ ìœ„í•´ ê°ì†Œ
        'train_ratio': 0.7,
        'val_ratio': 0.15,
        'test_ratio': 0.15,
        
        # ê¸°íƒ€ ì„¤ì •
        'early_stopping_patience': 8, # ì¡°ê¸ˆ ë” ë¹¨ë¦¬ ì¢…ë£Œ
        'save_frequency': 3,
        'save_dir': 'emotion/checkpoints_finetune',  # ìƒˆ í´ë”
        'use_class_weights': False
    }
    
    return config

def main():
    """ë©”ì¸ í›ˆë ¨ í•¨ìˆ˜"""
    print("ğŸ• ê°•ì•„ì§€ ê°ì • ë¶„ë¥˜ ëª¨ë¸ í›ˆë ¨ ì‹œì‘!")
    print("=" * 60)
    
    # ì„¤ì • ë¡œë“œ
    config = create_training_config()
    
    # ì„¤ì • ì¶œë ¥
    print("âš™ï¸  í›ˆë ¨ ì„¤ì •:")
    for key, value in config.items():
        print(f"   {key}: {value}")
    print()
    
    try:
        # ë°ì´í„°ì…‹ ë° DataLoader ìƒì„±
        import kagglehub
        dataset_path = kagglehub.dataset_download("danielshanbalico/dog-emotion")
        dataset_path = Path(dataset_path) / "Dog Emotion"
        csv_file = dataset_path / "labels.csv"
        
        print(f"ğŸ“ ë°ì´í„°ì…‹ ê²½ë¡œ: {dataset_path}")
        
        # DataLoader ìƒì„±
        train_loader, val_loader, test_loader, class_weights, emotion_to_idx = create_data_loaders(
            csv_file=csv_file,
            root_dir=dataset_path,
            batch_size=config['batch_size'],
            num_workers=config['num_workers'],
            image_size=config['image_size'],
            train_ratio=config['train_ratio'],
            val_ratio=config['val_ratio'],
            test_ratio=config['test_ratio']
        )
        
        # í›ˆë ¨ê¸° ìƒì„±
        trainer = DogEmotionTrainer(config)
        
        # í›ˆë ¨ ì‹œì‘
        history = trainer.train(train_loader, val_loader)
        
        print("\nğŸŠ í›ˆë ¨ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        return trainer, history
        
    except Exception as e:
        print(f"âŒ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return None, None

if __name__ == "__main__":
    trainer, history = main()